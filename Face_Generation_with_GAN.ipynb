{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 37705,
          "sourceType": "datasetVersion",
          "datasetId": 29561
        }
      ],
      "dockerImageVersionId": 30823,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Face Generation with GAN",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "jessicali9530_celeba_dataset_path = kagglehub.dataset_download('jessicali9530/celeba-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "YkIqX1_b_sod"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "IMAGE_SHAPE = (128, 128, 3)\n",
        "NOISE_DIM = 100\n",
        "BATCH_SIZE = 128\n",
        "EPOCHS = 50\n",
        "DATA_PATH = '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'\n",
        "\n",
        "def preprocess_image(file_path):\n",
        "    img_raw = tf.io.read_file(file_path)\n",
        "    img = tf.image.decode_jpeg(img_raw, channels=3)\n",
        "    img = tf.image.resize(img, (IMAGE_SHAPE[0], IMAGE_SHAPE[1]))\n",
        "    img = (img / 127.5) - 1.0  # Normalize to [-1, 1]\n",
        "    return img\n",
        "\n",
        "def load_celeba_images(path):\n",
        "    all_files = [os.path.join(path, f) for f in os.listdir(path)[:10000] if f.endswith('.jpg')]\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(all_files)\n",
        "    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    images_list = []\n",
        "    for batch in tqdm(dataset.batch(1024).prefetch(tf.data.AUTOTUNE)):\n",
        "        images_list.append(batch)\n",
        "\n",
        "    return tf.concat(images_list, axis=0)\n",
        "\n",
        "images = load_celeba_images(DATA_PATH)\n",
        "dataset = tf.data.Dataset.from_tensor_slices(images) \\\n",
        "                         .shuffle(buffer_size=50000) \\\n",
        "                         .batch(BATCH_SIZE) \\\n",
        "                         .prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-02T05:28:30.458162Z",
          "iopub.execute_input": "2025-03-02T05:28:30.458573Z",
          "iopub.status.idle": "2025-03-02T05:28:44.311333Z",
          "shell.execute_reply.started": "2025-03-02T05:28:30.458534Z",
          "shell.execute_reply": "2025-03-02T05:28:44.310162Z"
        },
        "id": "F9Vy9eeF_sog"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def build_generator(NOISE_DIM=NOISE_DIM):\n",
        "    model = tf.keras.Sequential([\n",
        "        # 1) Start with a dense layer that reshapes to 8x8 with 512 channels\n",
        "        layers.Dense(8*8*512, use_bias=False, input_shape=(NOISE_DIM,)),\n",
        "        layers.Reshape((8, 8, 512)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "\n",
        "        # 2) Upsample to 16x16\n",
        "        layers.Conv2DTranspose(256, (4,4), strides=(2,2), padding='same', use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "\n",
        "        # 3) Upsample to 32x32\n",
        "        layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "\n",
        "        # 4) Upsample to 64x64\n",
        "        layers.Conv2DTranspose(64, (4,4), strides=(2,2), padding='same', use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "\n",
        "        # 5) Upsample to 128x128\n",
        "        layers.Conv2DTranspose(32, (4,4), strides=(2,2), padding='same', use_bias=False),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "\n",
        "        # 6) Final layer â†’ 128x128x3, use tanh for output in [-1,1]\n",
        "        layers.Conv2DTranspose(3, (4,4), strides=(1,1), padding='same', use_bias=False, activation='tanh')\n",
        "    ], name=\"Generator\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_discriminator():\n",
        "    model = tf.keras.Sequential([\n",
        "        # 1) Downsample from 128x128 to 64x64\n",
        "        layers.Conv2D(64, (4,4), strides=(2,2), padding='same',\n",
        "                      input_shape=(128,128,3)),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        # 2) Downsample to 32x32\n",
        "        layers.Conv2D(128, (4,4), strides=(2,2), padding='same'),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        # 3) Downsample to 16x16\n",
        "        layers.Conv2D(256, (4,4), strides=(2,2), padding='same'),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        # 4) Downsample to 8x8\n",
        "        layers.Conv2D(512, (4,4), strides=(2,2), padding='same'),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        # 5) Flatten to a single logit\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(1)\n",
        "    ], name=\"Discriminator\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-02T05:28:44.332098Z",
          "iopub.execute_input": "2025-03-02T05:28:44.332317Z",
          "iopub.status.idle": "2025-03-02T05:28:44.34201Z",
          "shell.execute_reply.started": "2025-03-02T05:28:44.332295Z",
          "shell.execute_reply": "2025-03-02T05:28:44.341029Z"
        },
        "id": "48TG9WQ__soh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    return real_loss + fake_loss\n",
        "\n",
        "gen_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "disc_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-02T05:28:45.639278Z",
          "iopub.execute_input": "2025-03-02T05:28:45.639635Z",
          "iopub.status.idle": "2025-03-02T05:28:45.801787Z",
          "shell.execute_reply.started": "2025-03-02T05:28:45.639606Z",
          "shell.execute_reply": "2025-03-02T05:28:45.800788Z"
        },
        "id": "_Iq9qCtX_soj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(real_images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        # Ensure shapes match\n",
        "        assert generated_images.shape[1:] == IMAGE_SHAPE, \"Generated image shape mismatch!\"\n",
        "        assert real_images.shape[1:] == IMAGE_SHAPE, \"Real image shape mismatch!\"\n",
        "\n",
        "        real_output = discriminator(real_images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        g_loss = generator_loss(fake_output)\n",
        "        d_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(g_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(d_loss, discriminator.trainable_variables)\n",
        "\n",
        "    gen_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    return g_loss, d_loss\n",
        "\n",
        "\n",
        "def generate_and_show_images(noise):\n",
        "    # Generate images\n",
        "    preds = generator(noise, training=False)\n",
        "    preds = (preds + 1) / 2.0  # Shift from [-1,1] to [0,1]\n",
        "\n",
        "    # Plot\n",
        "    fig = plt.figure(figsize=(4,4))\n",
        "    for i in range(preds.shape[0]):\n",
        "        plt.subplot(4,4,i+1)\n",
        "        plt.imshow(preds[i])\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-02T05:28:46.761059Z",
          "iopub.execute_input": "2025-03-02T05:28:46.761382Z",
          "iopub.status.idle": "2025-03-02T05:28:46.768872Z",
          "shell.execute_reply.started": "2025-03-02T05:28:46.761356Z",
          "shell.execute_reply": "2025-03-02T05:28:46.767721Z"
        },
        "id": "U6i8ouTR_sok"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# def train(dataset, epochs):\n",
        "#     fixed_noise = tf.random.normal([16, NOISE_DIM])  # Fixed noise for generating sample images\n",
        "#     total_start_time = time.time()  # Start time for total training\n",
        "\n",
        "#     for epoch in range(epochs):\n",
        "#         epoch_start_time = time.time()  # Start time for the current epoch\n",
        "\n",
        "#         g_losses = []\n",
        "#         d_losses = []\n",
        "\n",
        "#         for image_batch in dataset:\n",
        "#             g_loss, d_loss = train_step(image_batch)\n",
        "#             g_losses.append(g_loss)\n",
        "#             d_losses.append(d_loss)\n",
        "\n",
        "#         # Calculate average losses for the epoch\n",
        "#         avg_g_loss = tf.reduce_mean(g_losses)\n",
        "#         avg_d_loss = tf.reduce_mean(d_losses)\n",
        "\n",
        "#         # Time taken for the epoch\n",
        "#         epoch_end_time = time.time()\n",
        "#         epoch_duration = epoch_end_time - epoch_start_time\n",
        "\n",
        "#         # Print epoch details\n",
        "#         print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "#               f\"Generator loss: {avg_g_loss:.4f}, \"\n",
        "#               f\"Discriminator loss: {avg_d_loss:.4f} | \"\n",
        "#               f\"Time: {epoch_duration:.2f} seconds\")\n",
        "\n",
        "#         # Generate and show images for the current epoch\n",
        "#         generate_and_show_images(fixed_noise)\n",
        "\n",
        "#     # Total time taken for training\n",
        "#     total_end_time = time.time()\n",
        "#     total_duration = total_end_time - total_start_time\n",
        "#     print(f\"\\nTotal Training Time: {total_duration:.2f} seconds\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-02T05:28:49.27809Z",
          "iopub.execute_input": "2025-03-02T05:28:49.278456Z",
          "iopub.status.idle": "2025-03-02T05:28:49.284763Z",
          "shell.execute_reply.started": "2025-03-02T05:28:49.278426Z",
          "shell.execute_reply": "2025-03-02T05:28:49.283762Z"
        },
        "id": "AXCW_XE1_som"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\", \"/gpu:1\"])\n",
        "\n",
        "with strategy.scope():\n",
        "    generator = build_generator()\n",
        "    discriminator = build_discriminator()\n",
        "\n",
        "    cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "    def generator_loss(fake_output):\n",
        "        return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "    def discriminator_loss(real_output, fake_output):\n",
        "        real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "        fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "        return real_loss + fake_loss\n",
        "\n",
        "    gen_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "    disc_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "# Existing dataset creation\n",
        "dataset = tf.data.Dataset.from_tensor_slices(images).shuffle(10000).batch(BATCH_SIZE)\n",
        "\n",
        "# Make a distributed dataset\n",
        "dist_dataset = strategy.experimental_distribute_dataset(dataset)\n",
        "\n",
        "@tf.function\n",
        "\n",
        "def distributed_train_step(dataset_inputs):\n",
        "    def step_fn(real_images):\n",
        "        return train_step(real_images)  # The train_step you already have\n",
        "\n",
        "    per_replica_g_loss, per_replica_d_loss = strategy.run(step_fn, args=(dataset_inputs,))\n",
        "    mean_g_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_g_loss, axis=None)\n",
        "    mean_d_loss = strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_d_loss, axis=None)\n",
        "    return mean_g_loss, mean_d_loss\n",
        "\n",
        "\n",
        "def train(dataset, epochs):\n",
        "    fixed_noise = tf.random.normal([16, NOISE_DIM])\n",
        "    total_start_time = time.time()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start_time = time.time()\n",
        "\n",
        "        g_losses = []\n",
        "        d_losses = []\n",
        "\n",
        "        for image_batch in dist_dataset:  # Use distributed dataset\n",
        "            g_loss, d_loss = distributed_train_step(image_batch)\n",
        "            g_losses.append(g_loss)\n",
        "            d_losses.append(d_loss)\n",
        "\n",
        "        avg_g_loss = tf.reduce_mean(g_losses)\n",
        "        avg_d_loss = tf.reduce_mean(d_losses)\n",
        "\n",
        "        epoch_duration = time.time() - epoch_start_time\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | \"\n",
        "              f\"Generator loss: {avg_g_loss:.4f}, \"\n",
        "              f\"Discriminator loss: {avg_d_loss:.4f} | \"\n",
        "              f\"Time: {epoch_duration:.2f} s\")\n",
        "\n",
        "        generate_and_show_images(fixed_noise)\n",
        "\n",
        "    total_duration = time.time() - total_start_time\n",
        "    print(f\"\\nTotal Training Time: {total_duration:.2f} seconds\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-02T05:28:51.006957Z",
          "iopub.execute_input": "2025-03-02T05:28:51.007274Z",
          "iopub.status.idle": "2025-03-02T05:28:53.189052Z",
          "shell.execute_reply.started": "2025-03-02T05:28:51.007252Z",
          "shell.execute_reply": "2025-03-02T05:28:53.18821Z"
        },
        "id": "nk-RKbON_son"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train(dataset, 100)\n",
        "generator.save('generator_800.h5')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-02T16:51:41.90116Z",
          "iopub.execute_input": "2025-03-02T16:51:41.90151Z",
          "execution_failed": "2025-03-02T16:59:23.661Z"
        },
        "id": "1KA0oa1C_son"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}